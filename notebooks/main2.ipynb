{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from shapely import wkt\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def calculate_area(wkt_polygon):\\n    polygon = wkt.loads(wkt_polygon)\\n    return polygon.area\\n     \\n      \\n    This function is deprecated'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def calculate_area(wkt_polygon):\n",
    "    polygon = wkt.loads(wkt_polygon)\n",
    "    return polygon.area\n",
    "     \n",
    "      \n",
    "    This function is deprecated\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '1232cd7984e543e188eebab0f8d6956f'\n",
    "\n",
    "def get_coordinates(county, name):\n",
    "    place_name = f\"{name}, {county} County, Florida\"\n",
    "    url = f\"https://api.opencagedata.com/geocode/v1/json?q={place_name}&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data['results']:\n",
    "        location = data['results'][0]['geometry']\n",
    "        return (location['lat'], location['lng'])\n",
    "    else:\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is gonna get our coordenates using the OpenCage Geocoding API using a key provided by one of the data scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(county, name, state=\"Florida\"):\n",
    "    place_name = f\"{name}, {county} County, {state}\"\n",
    "    url = f\"https://api.opencagedata.com/geocode/v1/json?q={place_name}&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data['results']:\n",
    "        location = data['results'][0]['geometry']\n",
    "        return (location['lat'], location['lng'])\n",
    "    else:\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(lat1, lon1, lat2, lon2):\n",
    "    # This function calculates the manhattan distance between two places\n",
    "    return abs(lat2 - lat1) + abs(lon2 - lon1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WKT</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>NAME</th>\n",
       "      <th>created_user</th>\n",
       "      <th>created_date</th>\n",
       "      <th>last_edited_user</th>\n",
       "      <th>last_edited_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON Z ((-9698711.156 3546590.3287 0,-96987...</td>\n",
       "      <td>ESCAMBIA</td>\n",
       "      <td>UNSURVEYED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLYGON Z ((-9061671.5384 3555608.0978 0,-9061...</td>\n",
       "      <td>DUVAL</td>\n",
       "      <td>HANNA PARK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POLYGON Z ((-9054509.0537 3514807.6314 0,-9054...</td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td>GUANA RIVER SP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLYGON Z ((-9668169.2045 3552697.6667 0,-9668...</td>\n",
       "      <td>ESCAMBIA</td>\n",
       "      <td>UNSURVEYED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLYGON Z ((-9597884.3653 3547578.4878 0,-9597...</td>\n",
       "      <td>WALTON</td>\n",
       "      <td>WALTON COUNTY BCHS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>POLYGON Z ((-9039717.3304 2937286.6217 0,-9039...</td>\n",
       "      <td>MONROE</td>\n",
       "      <td>ENP (HIGHLAND BEAC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>POLYGON Z ((-9039593.2653 2937286.6434 0,-9039...</td>\n",
       "      <td>MONROE</td>\n",
       "      <td>ENP (HIGHLAND BEAC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>POLYGON Z ((-9015596.0256 2844170.7527 0,-9015...</td>\n",
       "      <td>MONROE</td>\n",
       "      <td>LITTLE CRAWL KEY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>POLYGON Z ((-9016208.8296 2843020.5594 0,-9016...</td>\n",
       "      <td>MONROE</td>\n",
       "      <td>FAT DEER KEY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>POLYGON Z ((-8927112.0082 2930542.2383 0,-8927...</td>\n",
       "      <td>DADE</td>\n",
       "      <td>BISCAYNE NATIONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   WKT    COUNTY  \\\n",
       "0    POLYGON Z ((-9698711.156 3546590.3287 0,-96987...  ESCAMBIA   \n",
       "1    POLYGON Z ((-9061671.5384 3555608.0978 0,-9061...     DUVAL   \n",
       "2    POLYGON Z ((-9054509.0537 3514807.6314 0,-9054...  ST JOHNS   \n",
       "3    POLYGON Z ((-9668169.2045 3552697.6667 0,-9668...  ESCAMBIA   \n",
       "4    POLYGON Z ((-9597884.3653 3547578.4878 0,-9597...    WALTON   \n",
       "..                                                 ...       ...   \n",
       "297  POLYGON Z ((-9039717.3304 2937286.6217 0,-9039...    MONROE   \n",
       "298  POLYGON Z ((-9039593.2653 2937286.6434 0,-9039...    MONROE   \n",
       "299  POLYGON Z ((-9015596.0256 2844170.7527 0,-9015...    MONROE   \n",
       "300  POLYGON Z ((-9016208.8296 2843020.5594 0,-9016...    MONROE   \n",
       "301  POLYGON Z ((-8927112.0082 2930542.2383 0,-8927...      DADE   \n",
       "\n",
       "                   NAME  created_user  created_date  last_edited_user  \\\n",
       "0            UNSURVEYED           NaN           NaN               NaN   \n",
       "1            HANNA PARK           NaN           NaN               NaN   \n",
       "2        GUANA RIVER SP           NaN           NaN               NaN   \n",
       "3            UNSURVEYED           NaN           NaN               NaN   \n",
       "4    WALTON COUNTY BCHS           NaN           NaN               NaN   \n",
       "..                  ...           ...           ...               ...   \n",
       "297  ENP (HIGHLAND BEAC           NaN           NaN               NaN   \n",
       "298  ENP (HIGHLAND BEAC           NaN           NaN               NaN   \n",
       "299    LITTLE CRAWL KEY           NaN           NaN               NaN   \n",
       "300        FAT DEER KEY           NaN           NaN               NaN   \n",
       "301   BISCAYNE NATIONAL           NaN           NaN               NaN   \n",
       "\n",
       "     last_edited_date  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "..                ...  \n",
       "297               NaN  \n",
       "298               NaN  \n",
       "299               NaN  \n",
       "300               NaN  \n",
       "301               NaN  \n",
       "\n",
       "[302 rows x 7 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('florida-beach-names.csv')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[ds['NAME'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing thats gonna happen is we gonna get rid of the empty columns and the 3 null rows since non of that is gonna give any useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a copy of the data to work on that\n",
    "df = ds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['created_user', 'created_date', 'last_edited_user', 'last_edited_date', 'WKT'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we dont have any nan rows in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now based on our info of the beaches we are gonna find their approximate latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['latitude'] = None\n",
    "df['longitude'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the two empty columns holding the coordenates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    county = row['COUNTY']\n",
    "    name = row['NAME']\n",
    "    coordinates = get_coordinates(county, name)\n",
    "    df.at[index, 'latitude'] = coordinates[0]\n",
    "    df.at[index, 'longitude'] = coordinates[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also noticed some beaches sharing the same name in the same county, so we are gonna drop those and keep just one for each county since it will create the same entries of location with our geolocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['NAME']!='UNSURVEYED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['COUNTY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can notice how maybe some counties are repeated, SARASOTA and SARASOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df['COUNTY'] == 'SARASOTA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df['COUNTY']=='SARASOAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[filtered_df['COUNTY']!= 'SARASOAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df.duplicated(subset='NAME', keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the coordinates of duplicate beach names on a map shows that the following rows are irrelavant (inland) and can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop([63, 30, 113, 109, 85, 147, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be a misspelling so gonna get rid of that line since is already in the other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df['coordinates'] = list(zip(filtered_df['latitude'], filtered_df['longitude']))\n",
    "filtered_df = filtered_df.drop(['latitude', 'longitude'], axis=1)\n",
    "filtered_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df[filtered_df.coordinates.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop_duplicates(subset='coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('updated_beaches.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the data so it can be used later in the script without going thru all this changes again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv('updated_beaches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_location = get_coordinates('1200 Anastasia Ave', 'Coral Gables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test location. Soon to be filled with the txt value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_location_list = ['Coral Gables', 'Starting Location', starting_location[0], starting_location[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.loc[len(df.index)] = starting_location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df[(filtered_df.COUNTY == 'BOWARD') | (filtered_df.COUNTY == 'BROWARD')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for index, beach in df.iterrows():\n",
    "    if beach['NAME'] != 'Starting Location':\n",
    "        dist = manhattan_distance(starting_location[0], starting_location[1], beach['latitude'], beach['longitude'])\n",
    "        distances.append(dist)\n",
    "\n",
    "filtered_df['distances'] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def manhattan_distance(lat1, lon1, lat2, lon2):\n",
    "    return abs(lat2 - lat1) + abs(lon2 - lon1)\n",
    "\n",
    "# Calculate the distance matrix\n",
    "n = len(beaches_df)\n",
    "dist_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i != j:\n",
    "            dist_matrix[i, j] = manhattan_distance(beaches_df.loc[i, 'latitude'], beaches_df.loc[i, 'longitude'],\n",
    "                                                   beaches_df.loc[j, 'latitude'], beaches_df.loc[j, 'longitude'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & Scraping Data (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import random\n",
    "from itertools import permutations\n",
    "\n",
    "# API key to gain access to Open Cage Geo Data\n",
    "API_KEY = '1232cd7984e543e188eebab0f8d6956f'\n",
    "\n",
    "# Function to get coordinates\n",
    "def get_coordinates(county, name, state=\"Florida\"):\n",
    "    place_name = f\"{name}, {county} County, {state}\"\n",
    "    url = f\"https://api.opencagedata.com/geocode/v1/json?q={place_name}&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data['results']:\n",
    "        location = data['results'][0]['geometry']\n",
    "        return (location['lat'], location['lng'])\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "# Load dataset\n",
    "ds = pd.read_csv('florida-beach-names.csv')\n",
    "\n",
    "# Make copy of dataframe\n",
    "df = ds.copy()\n",
    "\n",
    "# Preprocessing steps - dropping null values\n",
    "df.drop(columns=['created_user', 'created_date', 'last_edited_user', 'last_edited_date', 'WKT'], inplace = True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Add latitude and longitude data\n",
    "df['latitude'] = None\n",
    "df['longitude'] = None\n",
    "for index, row in df.iterrows():\n",
    "    county = row['COUNTY']\n",
    "    name = row['NAME']\n",
    "    coordinates = get_coordinates(county, name)\n",
    "    df.at[index, 'latitude'] = coordinates[0]\n",
    "    df.at[index, 'longitude'] = coordinates[1]\n",
    "\n",
    "# Zip this data into coordinates\n",
    "filtered_df['coordinates'] = list(zip(filtered_df['latitude'], filtered_df['longitude']))\n",
    "filtered_df = filtered_df.drop(['latitude', 'longitude'], axis=1)\n",
    "\n",
    "# Deal with duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "filtered_df = df[df['NAME']!='UNSURVEYED'] # Missing name\n",
    "filtered_df = filtered_df[filtered_df['COUNTY']!= 'SARASOAT'] # Typo\n",
    "filtered_df = filtered_df.drop([63, 30, 113, 109, 85, 147, 32]) # Duplicate names, not actually beach locations\n",
    "filtered_df = filtered_df.drop_duplicates(subset='coordinates') # Drop duplicate coordinates\n",
    "\n",
    "# Save preprocessed dataset to csv\n",
    "filtered_df.reset_index(drop=True)\n",
    "filtered_df.to_csv('updated_beaches.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"optimal_route_sequence\": [\n",
      "        \"HANNA PARK\",\n",
      "        \"PONTE VEDRA N\",\n",
      "        \"S COUNTY BCHS\",\n",
      "        \"L TALBOT ISL SP\",\n",
      "        \"ST VINCENT NWR\",\n",
      "        \"DOG ISL\",\n",
      "        \"ST GEO ISL SP\",\n",
      "        \"ST GEO ISL\"\n",
      "    ],\n",
      "    \"distances_between_beaches\": [\n",
      "        9.1,\n",
      "        22.35,\n",
      "        0.41,\n",
      "        9.43,\n",
      "        8.89,\n",
      "        0.05,\n",
      "        1.6\n",
      "    ],\n",
      "    \"total_distance\": 51.83\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import random\n",
    "from itertools import permutations\n",
    "\n",
    "def preprocess(file_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocesses csv dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Zip this data into coordinates\n",
    "    filtered_df['coordinates'] = list(zip(filtered_df['latitude'], filtered_df['longitude']))\n",
    "    filtered_df = filtered_df.drop(['latitude', 'longitude'], axis=1)\n",
    "    \n",
    "    # Deal with duplicates\n",
    "    filtered_df = filtered_df.drop([63, 30, 113, 109, 85, 147, 32]) # Duplicate names, not actually beach locations\n",
    "    filtered_df.drop_duplicates(inplace=True)\n",
    "    filtered_df = filtered_df[filtered_df['NAME']!='UNSURVEYED'] # Missing name\n",
    "    filtered_df = filtered_df[filtered_df['COUNTY']!= 'SARASOAT'] # Typo\n",
    "    filtered_df = filtered_df.drop_duplicates(subset='coordinates') # Drop duplicate coordinates\n",
    "\n",
    "    return filtered_df\n",
    "    \n",
    "\n",
    "# Function to calculate the nearest n beaches from the starting beach\n",
    "def calculate_nearest_beaches(df, starting_beach, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate nearest n beaches from starting beach, using geodesic distance between lat/long coordinates.\n",
    "    \n",
    "    Inputs: \n",
    "        - df: DataFrame of all beach data, \n",
    "        - starting_beach: string, precise name of starting beach according to provided df\n",
    "        - n: int, number of nearest beaches to calculate\n",
    "\n",
    "    Output:\n",
    "        - DataFrame containing name of beach and geodesic distance (in miles) from starting beach\n",
    "    \"\"\"\n",
    "    \n",
    "    starting_beach_coord = df[df.NAME == starting_beach.upper()].coordinates.iloc[0]\n",
    "    df = df.copy()\n",
    "    df['geodesic_distance'] = df['coordinates'].apply(lambda x: geodesic(starting_beach_coord, x).miles)\n",
    "    df = df.sort_values(by='geodesic_distance', ascending=True).head(n + 1)\n",
    "    return df[['NAME', 'coordinates', 'geodesic_distance']]\n",
    "\n",
    "# Function to calculate the distance matrix\n",
    "def calculate_distance_matrix(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate distance matrix between every point in a dataframe.\n",
    "\n",
    "    Input:\n",
    "        - Dataframe, including coordinates\n",
    "    Output:\n",
    "        - Distance matrix (list of lists, including distance from each point to every other point)\n",
    "    \"\"\"\n",
    "    \n",
    "    coords = df['coordinates'].tolist()\n",
    "    distance_matrix = squareform(pdist(coords, lambda u, v: geodesic(u, v).miles))\n",
    "    return distance_matrix\n",
    "\n",
    "# Nearest Neighbor Algorithm to find the optimal route and calculate total distance\n",
    "def nearest_neighbor_algorithm(distance_matrix):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses Nearest Neighbor algorithm to find a good route.\n",
    "\n",
    "    Input: \n",
    "        - Distance matrix (list of lists, including distance from each point to every other point)\n",
    "    Output:\n",
    "        - Route (list of location names)\n",
    "        - Total distance (miles)\n",
    "        - Distances (list of distances between each location, miles)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(distance_matrix)\n",
    "    visited = [False] * n\n",
    "    route = [0]  # Start from the initial location\n",
    "    visited[0] = True\n",
    "    total_distance = 0.0\n",
    "    distances = []\n",
    "\n",
    "    for _ in range(1, n):\n",
    "        last_visited = route[-1]\n",
    "        next_city = np.argmin([distance_matrix[last_visited][j] if not visited[j] else float('inf') for j in range(n)])\n",
    "        route.append(next_city)\n",
    "        visited[next_city] = True\n",
    "        distance = round(distance_matrix[last_visited][next_city], 2)\n",
    "        distances.append(distance)\n",
    "        total_distance += distance\n",
    "\n",
    "    return route, total_distance, distances\n",
    "    \n",
    "def calculate_random_route(distance_matrix):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses randomization to calculate a baseline route.\n",
    "\n",
    "    Input: \n",
    "        - Distance matrix (list of lists, including distance from each point to every other point)\n",
    "    Output:\n",
    "        - Route (list of location names)\n",
    "        - Total distance (miles)\n",
    "        - Distances (list of distances between each location, miles)    \n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(distance_matrix)\n",
    "    route = list(range(1, n))  # Start from the second location\n",
    "    random.shuffle(route)\n",
    "    route = [0] + route  # Add the starting location at the beginning\n",
    "    total_distance = 0.0\n",
    "    distances = []\n",
    "\n",
    "    for i in range(n - 1):\n",
    "        distance = distance_matrix[route[i]][route[i + 1]]\n",
    "        distances.append(f\"{distance:.2f}\")\n",
    "        total_distance += distance\n",
    "\n",
    "    return route, total_distance, distances\n",
    "\n",
    "# Brute Force Algorithm to find the optimal route and calculate total distance\n",
    "def calculate_brute_force_route(distance_matrix):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses a brute force approach to find the best route.\n",
    "\n",
    "    Input: \n",
    "        - Distance matrix (list of lists, including distance from each point to every other point)\n",
    "    Output:\n",
    "        - Route (list of location names)\n",
    "        - Total distance (miles)\n",
    "        - Distances (list of distances between each location, miles)   \n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(distance_matrix)\n",
    "    min_distance = float('inf')\n",
    "    best_route = None\n",
    "    best_distances = []\n",
    "\n",
    "    for perm in permutations(range(1, n)):\n",
    "        current_route = [0] + list(perm)\n",
    "        current_distance = 0.0\n",
    "        distances = []\n",
    "\n",
    "        for i in range(n - 1):\n",
    "            distance = distance_matrix[current_route[i]][current_route[i + 1]]\n",
    "            distances.append(f\"{distance:.2f}\")\n",
    "            current_distance += distance\n",
    "\n",
    "        # Complete the route by returning to the starting point\n",
    "        total_distance = current_distance\n",
    "\n",
    "        if total_distance < min_distance:\n",
    "            min_distance = total_distance\n",
    "            best_route = current_route\n",
    "            best_distances = distances\n",
    "\n",
    "    return best_route, min_distance, best_distances\n",
    "\n",
    "# Main function to call the other functions and get the optimal route and total distance\n",
    "def calculate_route(df, starting_beach, n, algorithm='nearest neighbors'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate optimal route from start to finish, calling other functions. Choose between performance/efficiency levels.\n",
    "\n",
    "    Inputs: \n",
    "        - df: DataFrame of all location data\n",
    "        - starting_beach: string, precise name of starting location/beach according to provided df\n",
    "        - n: int, number of nearest locations to calculate. Refer to algorithm parameter for guidance on maximum size of n.\n",
    "        - algorithm: string, one of three options:\n",
    "            - 'nearest neighbors': best value of performance and efficiency (n has no upper limit)\n",
    "            - 'random': most efficient, but poor performance (baseline model, n has no upper limit)\n",
    "            - 'brute force': highest performance, lowest efficiency (n cannot exceed 9 without massive performance loss)\n",
    "    Outputs:\n",
    "        - optimal route: ordered list of locations, beginning with starting location\n",
    "        - total distance: float, total distance traveled from starting location to final location, miles\n",
    "        - distances: ordered list of distances between each location, miles\n",
    "    \"\"\"\n",
    "    \n",
    "    nearest_beaches = calculate_nearest_beaches(df, starting_beach, n)\n",
    "    distance_matrix = calculate_distance_matrix(nearest_beaches)\n",
    "    \n",
    "    if algorithm.lower() == 'nearest neighbors':\n",
    "        route_indices, total_distance, distances = nearest_neighbor_algorithm(distance_matrix)\n",
    "    if algorithm.lower() == 'random':\n",
    "        route_indices, total_distance, distances = calculate_random_route(distance_matrix)\n",
    "    if algorithm.lower() == 'brute force':\n",
    "        route_indices, total_distance, distances = calculate_brute_force_route(distance_matrix)\n",
    "    \n",
    "    optimal_route = nearest_beaches.iloc[route_indices]\n",
    "    \n",
    "    route = optimal_route.NAME.tolist()\n",
    "    distances = [float(d) for d in distances]  # Convert distances to float\n",
    "    \n",
    "    result = {\n",
    "        'optimal_route_sequence': route,\n",
    "        'distances_between_beaches': distances,\n",
    "        'total_distance': round(total_distance, 2)\n",
    "    }\n",
    "    \n",
    "    return json.dumps(result, indent=4)\n",
    "    \n",
    "    # print(f'Optimal route sequence: {route}\\n')\n",
    "    # print(f'Distance between each beach: {distances} miles.\\n')\n",
    "    # print(f'Total distance of route: {total_distance:.2f} miles.')\n",
    "    \n",
    "    # return optimal_route, total_distance, distances\n",
    "\n",
    "\n",
    "filtered_df = preprocess('updated_beaches.csv')\n",
    "# optimal_route, total_distance, distances = calculate_route(filtered_df, starting_beach='Hanna Park', n=7, algorithm='brute force')\n",
    "json_result = calculate_route(filtered_df, starting_beach='Hanna Park', n=7, algorithm='brute force')\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Google Maps Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyAGcEda7gNv_YZM95Z2a_6ioomdbUwOntI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from ortools.constraint_solver import pywrapcp\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'updated_beaches.csv'\n",
    "beaches_df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the starting beach\n",
    "starting_beach = beaches_df.iloc[0]\n",
    "\n",
    "# Extract coordinates for origins and destinations\n",
    "origins = [f\"{starting_beach['latitude']},{starting_beach['longitude']}\"]\n",
    "destinations = [f\"{row['latitude']},{row['longitude']}\" for _, row in beaches_df.iterrows() if row['NAME'] != starting_beach['NAME']]\n",
    "\n",
    "def get_distance_matrix(api_key, origins, destinations):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n",
    "    all_elements = []\n",
    "    \n",
    "    # Batch the destinations to avoid exceeding the limit\n",
    "    batch_size = 100 // len(origins)\n",
    "    for i in range(0, len(destinations), batch_size):\n",
    "        batch_destinations = destinations[i:i + batch_size]\n",
    "        params = {\n",
    "            \"origins\": \"|\".join(origins),\n",
    "            \"destinations\": \"|\".join(batch_destinations),\n",
    "            \"key\": api_key,\n",
    "            \"departure_time\": \"now\",\n",
    "            \"traffic_model\": \"best_guess\"\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        result = response.json()\n",
    "        \n",
    "        if result['status'] == 'OK':\n",
    "            all_elements.extend(result['rows'][0]['elements'])\n",
    "        else:\n",
    "            print(\"Error in API response:\", result['status'])\n",
    "            return None\n",
    "    \n",
    "    return all_elements\n",
    "\n",
    "# Your Google Maps API key\n",
    "api_key = 'YOUR_GOOGLE_MAPS_API_KEY'\n",
    "\n",
    "# Get distance matrix\n",
    "distance_elements = get_distance_matrix(api_key, origins, destinations)\n",
    "\n",
    "# Check if the distance_elements is None\n",
    "if distance_elements is None:\n",
    "    print(\"Error occurred while fetching distance matrix.\")\n",
    "else:\n",
    "    # Process the distance matrix to find the closest beaches\n",
    "    distances = [element['duration']['value'] for element in distance_elements]\n",
    "    beaches_df = beaches_df[beaches_df['NAME'] != starting_beach['NAME']]\n",
    "    beaches_df['travel_time'] = distances\n",
    "\n",
    "    # Find the ten closest beaches based on travel time\n",
    "    closest_beaches = beaches_df.nsmallest(10, 'travel_time')\n",
    "\n",
    "    # Display the closest beaches\n",
    "    print(closest_beaches)\n",
    "\n",
    "    # Create data model for TSP\n",
    "    def create_data_model(closest_beaches):\n",
    "        \"\"\"Stores the data for the problem.\"\"\"\n",
    "        data = {}\n",
    "        travel_times = closest_beaches['travel_time'].tolist()\n",
    "        num_beaches = len(travel_times) + 1\n",
    "        distance_matrix = [[0] * num_beaches for _ in range(num_beaches)]\n",
    "        for i in range(1, num_beaches):\n",
    "            distance_matrix[0][i] = travel_times[i-1]\n",
    "            distance_matrix[i][0] = travel_times[i-1]\n",
    "        data['distance_matrix'] = distance_matrix\n",
    "        data['num_vehicles'] = 1\n",
    "        data['depot'] = 0\n",
    "        return data\n",
    "\n",
    "    # Solve the TSP\n",
    "    def main():\n",
    "        closest_beaches = closest_beaches.reset_index()\n",
    "        data = create_data_model(closest_beaches)\n",
    "\n",
    "        # Create the routing index manager.\n",
    "        manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n",
    "                                               data['num_vehicles'], data['depot'])\n",
    "\n",
    "        # Create Routing Model.\n",
    "        routing = pywrapcp.RoutingModel(manager)\n",
    "\n",
    "        def distance_callback(from_index, to_index):\n",
    "            \"\"\"Returns the travel time between the two nodes.\"\"\"\n",
    "            from_node = manager.IndexToNode(from_index)\n",
    "            to_node = manager.IndexToNode(to_index)\n",
    "            return data['distance_matrix'][from_node][to_node]\n",
    "\n",
    "        transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n",
    "\n",
    "        routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n",
    "\n",
    "        search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "        search_parameters.first_solution_strategy = (\n",
    "            routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n",
    "\n",
    "        solution = routing.SolveWithParameters(search_parameters)\n",
    "\n",
    "        if solution:\n",
    "            print_solution(manager, routing, solution)\n",
    "\n",
    "    def print_solution(manager, routing, solution):\n",
    "        \"\"\"Prints solution on console.\"\"\"\n",
    "        print('Objective: {}'.format(solution.ObjectiveValue()))\n",
    "        index = routing.Start(0)\n",
    "        plan_output = 'Route:\\n'\n",
    "        route_distance = 0\n",
    "        while not routing.IsEnd(index):\n",
    "            plan_output += ' {} ->'.format(manager.IndexToNode(index))\n",
    "            previous_index = index\n",
    "            index = solution.Value(routing.NextVar(index))\n",
    "            route_distance += routing.GetArcCostForVehicle(previous_index, index, 0)\n",
    "        plan_output += ' {}\\n'.format(manager.IndexToNode(index))\n",
    "        print(plan_output)\n",
    "        print('Route distance: {}'.format(route_distance))\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def verify_api_key(api_key):\n",
    "    test_url = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n",
    "    params = {\n",
    "        \"origins\": \"30.370955,-81.402843\",\n",
    "        \"destinations\": \"29.912180,-81.409890\",\n",
    "        \"key\": api_key,\n",
    "    }\n",
    "    response = requests.get(test_url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "# Your Google Maps API key\n",
    "api_key = 'YOUR_GOOGLE_MAPS_API_KEY'\n",
    "\n",
    "# Verify the API key\n",
    "response = verify_api_key(api_key)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
